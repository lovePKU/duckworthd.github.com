<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title>Transmogrifier</title>
		<description>Changing for the better.</description>
		<link>http://www.transmogrifier.com</link>
		<atom:link href="http://www.transmogrifier.com/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Topic Models aren't hard</title>
				<description>Even you, yes YOU, can understand Topic Models.</description>
				<pubDate>Mon, 21 Jan 2013 00:00:00 -0500</pubDate>
				<link>http://www.transmogrifier.com/blog/topic-models.html</link>
				<guid isPermaLink="true">http://www.transmogrifier.com/blog/topic-models.html</guid>
			</item>
		
			<item>
				<title>Beginnings</title>
				<description>Where everything starts over...</description>
				<pubDate>Tue, 01 Jan 2013 00:00:00 -0500</pubDate>
				<link>http://www.transmogrifier.com/blog/beginnings.html</link>
				<guid isPermaLink="true">http://www.transmogrifier.com/blog/beginnings.html</guid>
			</item>
		
			<item>
				<title>ADMM: parallelizing convex optimization</title>
				<description>Stochastic Gradient Descent isn't the only option</description>
				<pubDate>Sun, 24 Jun 2012 00:00:00 -0400</pubDate>
				<link>http://www.transmogrifier.com/blog/admm.html</link>
				<guid isPermaLink="true">http://www.transmogrifier.com/blog/admm.html</guid>
			</item>
		
			<item>
				<title>Stochastic Gradient Descent and Sparse \(L_2\) regularization</title>
				<description>You don't have to trade sparsity for regularization</description>
				<pubDate>Thu, 10 May 2012 00:00:00 -0400</pubDate>
				<link>http://www.transmogrifier.com/blog/sparse-l2.html</link>
				<guid isPermaLink="true">http://www.transmogrifier.com/blog/sparse-l2.html</guid>
			</item>
		
	</channel>
</rss>
